<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RLVR</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400&family=Source+Code+Pro:wght@400;500&display=swap" rel="stylesheet">
    <!-- D3.js for plots -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
</head>
<body>
    <article>
        <header>
            <h1>RLVR Experiments</h1>
        </header>

        <p>
            Github repo: <a href="https://github.com/halflearned/rlvr-experiments">halflearned/rlvr-experiments</a>
        </p>

        <section class="intro">
            <p>The post has two parts. Part 1 gives an overview of the package and typical usage. Part 2 presents a few experiments we ran with it.</p>
        </section>

        <section id="system-architecture">
            <h3>System overview</h3>

            <p>The main type of problem this package focuses on is Monte Carlo-based RLVR algorithms such as Group Relative Policy Optimization (GRPO) and its variants. For this sort of problem, the canonical setup is a system that runs in a <b>producer-consumer pipeline</b>:
            </p>
            <p>The "producer" is a loop that samples prompts, generates completions using a copy of the model, verifies these completions, and puts all results in a buffer. The "consumer" pops from the buffer, waits until there are enough samples, batches samples together, and trains its own model copy. Once the model weights are updated, the trainer's new weights may be synced to the producer's weights to keep generations on policy.</p>
            <p>This sort of design, decoupling generation from training, has been used in <a href="https://arxiv.org/pdf/2501.12599">Kimi 1.5</a> and taken up by modern RLVR packages like <a href="https://pytorch.org/blog/introducing-torchforge/">torchforge</a>. Let's work through a typical flow.
            </p>
            <h3>Components</h3>

            <p>A typical GRPO-like flow will have the following components:</p>

            <ul>
                <li><strong>Trainer</strong> (TorchTitan): holds one copy of model weights, gets updated at every optimizer step</li>
                <li><strong>Rollout engine</strong> (vLLM): holds another copy of model weights, efficiently generates completions from prompts</li>
                <li><strong>Reference model</strong> (vLLM): computes log-probabilities of an anchor model, used to compute a penalty for moving too far away from the base model</li>
                <li><strong>Verifier pool</strong>: checks completions against ground truth, returns rewards</li>
                <li><strong>Data iterator</strong>: serves prompts, tracks which are pending/in-flight/done/failed</li>
            </ul>

            <p>The trainer's and vLLM engine's weights can be sharded across GPUs for different parallelism strategies. Weight sync happens periodically over NCCL, transferring updated policy weights from the trainer to the rollout and reference models.</p>
            <h3>Producer loop</h3>
            <p>
                The producer loop runs dozens of async workers concurrently, all competing for prompts from the data iterator. Each worker independently shepherds one prompt through the full pipeline: generation, verification, reference logprobs, then buffer.
            </p>
            <p>
                The producer has no visibility into the trainer's progress—it just keeps generating until the data is exhausted, pausing only during weight syncs. If a sample doesn't provide enough variation (all-wrong or all-correct) it is <em>filtered out</em> and not attempted again until the next epoch.
            </p>

            <figure>
                <img src="figures/producer.png" alt="Producer loop diagram">
            </figure>

            <details class="code-details">
                <summary>Producer code (<code>train_grpo_minimal.py</code>)</summary>
<pre><code class="language-python">async def produce_epoch():
    async def worker():
        while True:
            item = await data_iter.get_next_async()
            if item is None:
                return
            prompt_id = item["problem"]["prompt_id"]
            try:
                # Generate completions from current policy
                response = await rollout.generate_single(item["template"], **sampling_params)
                completions = [out.text for out in response.outputs]
                rollout_sample = RolloutSample.from_vllm(response, pad_token_id)

                # Score with verifier
                rewards = await verify(item["problem"], completions)

                # Skip if all rewards identical (no signal)
                if torch.tensor(rewards).std() &lt; 1e-6:
                    data_iter.mark_done(prompt_id)
                    return

                # Get reference model logprobs (for KL penalty)
                ref_logprobs = await compute_ref_logprobs(rollout_sample)

                # Push to buffer for consumer
                # Tag with trainer_version so consumer can check staleness later
                sample = TrainSample(rollout_sample, rewards, ref_logprobs,
                                     item_id=prompt_id, trainer_version=rollout.trainer_version)
                await buffer.put(sample, rollout.trainer_version, item_id=prompt_id)
            except Exception:
                data_iter.mark_failed(prompt_id)

    async with asyncio.TaskGroup() as tg:
        for _ in range(64):
            tg.create_task(worker())</code></pre>
            </details>

            <h3>Consumer loop</h3>
            <p>
                The consumer is a standard training loop: pull samples, batch them, forward/backward, optimizer step. Only two things are different. First, samples are checked for <em>staleness</em>: if the trainer has moved too far ahead of the weights used to generate a particular sample, that sample is (lazily) evicted and its prompt is re-queued for generation within this epoch. Second, after each optimizer step the trainer version increments, and consumed prompts are marked done so they won't be sampled again this epoch. Training usually happens in micro-batches that depend on the sample's sequence length.
            </p>

            <figure>
                <img src="figures/consumer.png" alt="Consumer loop diagram">
            </figure>

            <p>A wrinkle: moving data for both the forward and backward pass must happen within the same call.
            </p>

            <details class="code-details">
                <summary>Consumer code (<code>train_grpo_minimal.py</code>)</summary>
<pre><code class="language-python">for epoch in range(training.get("num_epochs", 1)):
    data_iter.new_epoch(seed=42 + epoch)
    producer = asyncio.create_task(produce_epoch())

    pending = []
    accum_count = 0

    while True:
        entry = await buffer.pop()
        if entry is None:
            break

        pending.append(entry.item)
        if len(pending) &lt; training["prompts_per_forward_backward"]:
            continue

        # ── Evict stale samples ────────────────────────────────────
        pending = evict_stale(pending)
        if len(pending) &lt; training["prompts_per_forward_backward"]:
            continue  # not enough left after eviction, keep collecting

        # ── Make batch ─────────────────────────────────────────────
        batch, stats = make_batch(pending, pad_token_id)
        group_sizes = [group_size] * len(pending)
        item_ids = [s.item_id for s in pending]
        pending = []

        # ── Compute advantages ────
        advantages = compute_grpo_advantages(batch.rewards, group_sizes=group_sizes)

        # ── Forward + backward (with micro-batching) ───────────────
        loss, _ = await trainer.forward_backward(
            loss_fn,
            batch.input_ids,
            loss_args=(batch.completion_ids, batch.ref_logprobs,
                       batch.logprobs, advantages),
            loss_kwargs={"padding_mask": batch.mask,
                         "prompt_lens": batch.prompt_lens,
                         "temperature": temperature},
            scale_loss=1.0 / accumulation_steps,
            micro_batch_size=get_micro_batch_size(stats.padded_seq_len),
        )

        # Mark items as consumed
        for item_id in item_ids:
            data_iter.mark_done(item_id)

        accum_count += 1
        if accum_count &lt; accumulation_steps:
            continue

        # ── Optimizer step ─────────────────────────────────────────
        grad_norm = await trainer.optim_step()
        accum_count = 0

        # ── Weight sync ──────────────────────────────────────────
        if trainer.version % sync_ref_every == 0:
            await sync_titan_to_vllm(trainer, reference,
                                     trainer_version=trainer.version)
        if trainer.version % sync_model_every == 0:
            await sync_titan_to_vllm(trainer, rollout,
                                     trainer_version=trainer.version)

        if training.get("max_steps") and trainer.version >= training["max_steps"]:
            break

    producer.cancel()
    await asyncio.gather(producer, return_exceptions=True)
    buffer.reset()</code></pre>
            </details>

            <h3>Weight sync</h3>
            <p>
                This is how the trainer's updated weights get to the rollout engine and reference model. The main challenge is copying efficiently when the trainer and rollout models may have weights sharded differently—the trainer might use FSDP-style sharding across 8 GPUs while the rollout uses tensor parallelism across 2. Different packages handle this differently. For example, <a href="#">torchforge</a> is developing a fast filesystem (<a href="https://github.com/meta-pytorch/torchstore">torchstore</a>) to save and load weights efficiently.
            </p>
            <p>
                Our strategy is to avoid disk entirely, relying on NCCL communication channels to broadcast weights directly between GPU memory. At initialization time, we create NCCL communicator groups (borrowing vLLM's StatelessProcessGroup class). This allows us to create overlapping "worlds" shared by each trainer-receiver pair. 
            </p>
            <ol>
                <li>Pause the rollout engine: block new requests behind an asyncio Event, and either wait for in-flight requests to drain (default) or abort them.</li>
                <li>Prepare the trainer for sync. Each trainer rank builds two things: a parameter map from HuggingFace parameter names to their local DTensor references, and a chunk plan—a list of (parameter name, offset, size) tuples dividing weights into ~100MB pieces. The chunk plan is sent to vLLM workers over Ray so both sides know how to pack and unpack.</li>
                <li>For each chunk: all trainer ranks participate in an all-gather to reconstruct full tensors from their shards (required because DTensor only exposes all-gather, not gather-to-one). Only rank 0 packs the result into a flat buffer and broadcasts over NCCL; vLLM workers receive into their own buffer and unpack into local model parameters, resharding as needed for their own parallelism layout.</li>
                <li>Update the rollout engine's trainer version so newly generated samples are tagged correctly.</li>
                <li>Resume generation: set the Event, blocked workers continue.</li>
            </ol>
            <p>
                With this strategy we complete the entire weight transfer for Qwen3-32B in under six seconds (or nine seconds if including waiting time for rollout engines to abort). This is a relatively small number relative to generation, since one vLLM engine takes over 60 seconds to produce 16 completions for 16 prompts. For comparison to other implementations, torchforge <a href="https://github.com/meta-pytorch/torchforge/issues/626">reportedly</a> takes about 58 seconds for the weight sync with torchstore.
            </p>

            <figure class="figure-stepper" id="weight-sync-stepper">
                <div class="stepper-viewport">
                    <img src="figures/weight-1.jpg" alt="Weight sync step 1" class="stepper-img active">
                    <img src="figures/weight-2.jpg" alt="Weight sync step 2" class="stepper-img">
                    <img src="figures/weight-3.jpg" alt="Weight sync step 3" class="stepper-img">
                    <img src="figures/weight-4.jpg" alt="Weight sync step 4" class="stepper-img">
                    <img src="figures/weight-5.jpg" alt="Weight sync step 5" class="stepper-img">
                </div>
                <div class="stepper-controls">
                    <button class="stepper-btn stepper-prev" aria-label="Previous">&larr;</button>
                    <span class="stepper-counter">1 / 5</span>
                    <button class="stepper-btn stepper-next" aria-label="Next">&rarr;</button>
                </div>
            </figure>

            <h3>Visualization</h3> We keep optional but extensive tracing capabilities that track the evolution of average rewards, generation speed, trainer effective MFU, and other quantities useful for debugging (max kl divergence, logprob ratio, etc). These are shown in our visualization tool <a href="#TODO">RLVR Heartbeat</a>.


            <h3>Production-level run</h3> We mock something closer to a larger, production-level run with Qwen3-32B (dense) using three 8xA100 80GB nodes. 

        </section>

        <section id="experiments">
            <h2>2. Basic experiment setup</h2>
            <p>
            All experiments use <a href="https://huggingface.co/Qwen/Qwen3-1.7B-Base">Qwen3-1.7B-Base</a> as the initial model, and at most one full 8xA100 80GB node.
            </p>
            <p>We'll train this model on simple math tasks (GSM8k), instruction-following (IFBench), and code (MBPP/APPS). We use an objective function of the form 
            $$\text{(RL objective)} - \alpha_{\text{SFT}} \cdot \text{(SFT loss)} - \beta_{\text{KL}} \text{(Divergence penalty)}$$
            </p>

            <p>
            Let's look at each in turn.
            </p>

            <h4>Reinforcement-learning objective</h4>  
            
            <p>For <a href="https://arxiv.org/abs/2503.20783">GRPO</a>, given a prompt $p$, we generate $G$ completions $o_{i}$ from the current model $\pi_{\theta}$, and compute:</p>

            $$
            J_{\text{GRPO}}(g)
            =
            \frac{1}{G} \sum_{i=1}^{G} \sum_{t=1}^{|o_i|}
            \left\{ \min \left[
                \frac{ \pi_{\theta}(o_t | q, o_{\lt t}) }{ \pi_{\theta_{\text{old}}}(o_t | q, o_{\lt t})  } \hat{A}_{i,t}
                ,
                \text{clip}\left(
                    \frac{ \pi_{\theta}(o_t | q, o_{\lt t}) }{ \pi_{\theta_{\text{old}}}(o_t | q, o_{\lt t})  },
                    1 - \varepsilon,
                    1 + \varepsilon
                 \right)
                \hat{A}_{i,t}
             \right] \right\}
            \tag{1}
            $$

            <p>
                where the advantage $\hat{A}_{i} = (r_i - \bar{r}) / \sigma_r$ is the z-scored reward for completion $i$ within its group, $\pi_{\theta_{\text{old}}}$ is the rollout policy used to generate the completions, and the probability ratio is clipped following <a href="https://arxiv.org/pdf/1707.06347">PPO-Clip</a>.
            </p>

            <p>
                Our second algorithm is a variant of <a href="https://arxiv.org/pdf/2503.14476">DAPO</a> (Decoupled Clip and Dynamic Sampling Policy Optimization), which uses the following objective,
            </p>

            $$
            J_{\text{DAPO}}(g)
            =
            \frac{1}{{\color{orange}\sum_{i} |o_i|}} \sum_{i=1}^{G} \sum_{t=1}^{|o_i|}
            \left\{ \min \left[
                \frac{ \pi_{\theta}(o_t | q, o_{\lt t}) }{ \pi_{\theta_{\text{old}}}(o_t | q, o_{\lt t})  } \hat{A}_{i,t}
                ,
                \text{clip}\left(
                    \frac{ \pi_{\theta}(o_t | q, o_{\lt t}) }{ \pi_{\theta_{\text{old}}}(o_t | q, o_{\lt t})  },
                    {\color{teal} 1 - \varepsilon_{\text{low}}},
                    {\color{teal} 1 + \varepsilon_{\text{high}}}
                 \right)
                \hat{A}_{i,t}
             \right] \right\}
            \tag{2}
            $$

            <p>
                The <span style="color: #e8a030;">normalization factor</span> divides by total tokens across the group rather than by $G$, removing a length bias that would otherwise discount longer completions. The <span style="color: teal;">clipping bounds</span> are asymmetric ($\varepsilon_{\text{low}} = 0.2$, $\varepsilon_{\text{high}} = 0.28$). The latter should technically allow the policy more room to increase probability on rewarded completions than to decrease it on penalized ones. However, since most of our experiments are strictly on-policy, the clipping matters little.
            </p>

            <p>
                In either implementation, when rewards are constant, we discard the entire prompt. In addition, we ensure that every batch has the same number of prompts per step.
            </p>


            <h4>KL penalty</h4>  
            <p>
                We keep a small penalty for divergence with respect to a frozen reference policy. This is implemented using the "<a href="http://joschu.net/blog/kl-approx.html">Schulman</a>" approximation to the KL divergence $D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$.
            </p>

            <h4>Supervised fine tuning</h4>    
            <p>
                To construct supervised fine-tuning targets, we first evaluate the base model on the training set using pass@128 sampling with temperature=1.0. We then filter to "hard" problems, defined as those where the base model's pass rate falls strictly between 0 and 0.5 — representing problems the model can occasionally solve but not reliably. For each qualifying problem, we select one random correct completion as the SFT target. This yields 2,742 examples from GSM8k (out of 7,473 training problems) and 12,245 examples from IFEval. This construction can be seen as a form of rejection sampling fine-tuning (see e.g., <a href="https://arxiv.org/pdf/2308.01825">Yuan et al., 2023</a>), and because the completions come from the base model rather than a stronger teacher, this is a form of self-distillation. During training, we draw from this offline dataset a batch of the same size as the one used for the RL objective. For all our experiments, the SFT term is weighted at $\alpha_{\text{SFT}} = 0.001$. We have not yet experimented with increasing the dataset.
            </p>

            <p>
                We have not experimented with adding an entropy bonus or other forms of reward shaping (e.g. length-based rewards).
            </p>

            <section id="gsm8k">
                <h3>2.2 GSM8K results</h3>
                <p>
                    We train on the GSM8K training set (n=7473). We set group size G=8, batch size 64, max completion tokens 512, and run for 200 steps. We ablate algorithm (DAPO vs GRPO), auxiliary SFT loss, learning rate, and max staleness. Our verifiers take the last number extracted in various ways (<code>\boxed{X}</code>, <code>The answer is X</code>, etc) and compare it to the ground truth via string matching (all GSM8k answers are integers).
                </p>
                <h4>Results</h4>
                <ul>
                    <li>
                        GRPO vs DAPO: For a given set of hyperparameters, both perform very similarly. With the more aggressive learning rate lr=1e-5, rewards increase faster during training and achieve our best results on the GSM8k test set. However, MATH ability degrades, in particular for DAPO. Also, entropy decreases more rapidly with a higher learning rate, but generic performance as measured by MMLU remains essentially unchanged.</li>
                    <li>
                        Adding SFT: The effect of the auxiliary SFT loss is somewhat ambiguous. For the more aggressive learning rate 1e-5, it mitigates some of the loss in MATH performance, in particular for DAPO. However, for the slower learning rate 5e-6 it results in a further decrease in MATH performance. Perhaps it works as a regularizer for more aggressive learning rates, preventing overfitting on GSM8k. It does not prevent fast entropy decay in either case.
                    </li>
                    <li>
                        Staleness: our system naturally allows for using older samples — this can be done by simply not evicting samples older than a certain value of max_staleness. In our code, <code>max_staleness = 0</code> means the model is strictly on-policy, and <code>max_staleness = n</code> allows for samples generated with weights from n optimization steps ago. When generation is fast relative to training, setting <code>max_staleness=1</code> often makes training run moderately faster (20% faster on GSM8k), because as soon as the sync ends the trainer can start consuming old data before any new generation has completed. When generation is the bottleneck, though, the speed gains are very small. Since our weight sync mechanism is quite fast even for relatively sizable models, there may be little need for this feature. Still, for completeness we tested the effect of increasing staleness in a limited manner.
                    </li>
                </ul>

                <!-- GSM8K interactive plots -->
                <div class="experiment-plots">
                    <div id="gsm8k-preset-buttons" class="preset-buttons"></div>
                    <div id="gsm8k-run-legend" class="run-legend"></div>
                    <div id="gsm8k-preset-explanation" class="preset-explanation"></div>

                    <h4>Rewards</h4>
                    <div class="plot-row plot-row-1x4">
                        <div class="plot-container" id="gsm8k-plot-reward"></div>
                        <div class="plot-container" id="gsm8k-plot-completion-len"></div>
                        <div class="plot-container" id="gsm8k-plot-allcorr"></div>
                        <div class="plot-container" id="gsm8k-plot-allwrong"></div>
                    </div>

                    <h4>Training Dynamics</h4>
                    <div class="plot-row plot-row-1x4">
                        <div class="plot-container" id="gsm8k-plot-loss-grpo"></div>
                        <div class="plot-container" id="gsm8k-plot-loss-sft"></div>
                        <div class="plot-container" id="gsm8k-plot-kl"></div>
                        <div class="plot-container" id="gsm8k-plot-entropy"></div>
                    </div>
                </div>

                <!-- GSM8K comparison table (auto-generated) -->
                <div id="gsm8k-comparison-table" class="results-table-container"></div>

                <ul>
                <li>
                    SFT-only: we also experimented with trying to improve the model using only SFT, without an RL component. In that case, the gains were fairly modest, and remained small even when we ran it for more epochs than we did for the RL runs.
                </li>
                <li>
                    Pass@k: There have been several papers noting that performance in math capability, as measured by pass@k for some large k, doesn't improve with RLVR; see e.g., <a href="https://arxiv.org/pdf/2504.13837">Yue, Chen et al., 2025</a>. This would suggest that RLVR isn't well-suited for forcing a base model to learn new skills, only to reinforce those that it already has — the "elicitation" theory. We also observe this here, as pass@128 seems to match the base model for both GSM8k and MATH for all model variations (see graph below). This may be a quirk of the Qwen family of models, as described <a href="https://www.interconnects.ai/p/reinforcement-learning-with-random">here</a>.
                </li>
                </ul>

                <div class="experiment-plots">
                    <h4>Pass@k (step 200)</h4>
                    <div class="plot-row plot-row-1x4">
                        <div class="plot-container" id="gsm8k-plot-passk-gsm8k"></div>
                        <div class="plot-container" id="gsm8k-plot-passk-math"></div>
                        <div class="plot-container" id="gsm8k-plot-passk-aime"></div>
                        <div class="plot-container" id="gsm8k-plot-passk-beyondaime"></div>
                    </div>
                </div>

            <h4>Going forward</h4>
                <ul>
                    <li>GSM8k is more or less saturated, further experiments should use GSM8k as a sanity check then move on to newer, harder math-related tasks such as <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset">DeepScaleR-40k</a>.
                    </li>
                    <li>We ran preliminary but unsuccessful experiments with two variations of the above. First, setting a "curriculum", based on decreasing pass-at-k for some moderate large k (32-128); second, allowing the model to sample until it has a certain number of successes/failures or reaches a maximum number of tries (following <a href="https://arxiv.org/abs/2510.04996">Reinforce-Ada</a>). These experiments might be worth revisiting with larger datasets or other models.
                    </li>
                    <li>We haven't explored certain post-training techniques such as a warm-start CoT training, or templates (including <code>&lt;think&gt;</code> tags), which are becoming standard in the literature.</li>
                    <li>
                        Other forms of distillations and self-distillation such as 
                        <a href="https://arxiv.org/pdf/2601.18779">Privileged on-policy guidance</a> and <a href="https://arxiv.org/pdf/2203.14465">Self-Taught Reasoner (STaR)</a> (more on this at the bottom of this report)
                    </li>

                </ul>

            </section>

            <section id="instruction-following">
                <h3>2.3 Instruction-following results</h3>
                <p>
                    In this section we train the model to follow verifiable formatting and structural constraints in prompts — such as "write exactly 3 paragraphs" or "include the keyword 'ocean' at least twice."
                </p>
                <p>
                    We train on about 40k examples from Ai2's <a href="https://huggingface.co/datasets/allenai/IF_multi_constraints_upto5">IFEval-multi-constraints</a> and evaluate on <a href="https://huggingface.co/datasets/google/IFEval">IFEval</a> and <a href="https://huggingface.co/datasets/allenai/IFBench_test">IFBench_test</a> (<a href="https://arxiv.org/pdf/2507.02833">Pyatkin et al, 2025</a>). There's no overlap in prompts across these three datasets, but the training set contains 25 constraints from IFEval and 29 new ones. IFBench_test has 58 new constraints that are entirely unseen.
                </p>
                <p>
                    Note that this instruction following is purely mechanical, and while training against it the model may learn to follow constraints while outputting gibberish (in fact, some of the constraints force the model away from typical natural language). Here's how <a href="https://chatgpt.com/share/6980456e-cb08-8005-bb85-ebe43b4a2b39">GPT 5.2</a> and <a href="https://claude.ai/share/a79b587a-20d5-4b66-ae67-f093a92a8d0d">Claude Opus 5.2</a> handle an example from IFBench.
                </p>
                <p>
                    All runs set group size G=16, max completion tokens 2048, batch size 128. We evaluate models at two granularities: "prompt-level" accuracy means all constraints in that prompt are satisfied; "instruction-level" accuracy is the fraction of individual instructions that were satisfied. During training, we use instruction-level accuracy so partial rewards are possible.
                </p>
            
                <h4>Results</h4>
                <ul>
                <li>
                    Overall, we can find combinations of hyperparameters that substantially improve instruction-following performance, as measured by our benchmarks.
                </li>
                <li>
                    We have not completed a full ablation of GRPO vs DAPO over a grid of all (lr, beta) values, but comparing like-to-like values their performance seems similar. As in GSM8k, completion length is slightly smaller for DAPO. They behave similarly in terms of entropy decay.
                </li>
                <li>
                    Similarly to the GSM8k runs, when SFT is introduced it seems to increase performance for higher learning rates.
                </li>
                <li>
                   Pass@k rates similarly trend toward convergence with the base model.
                </li>
                </ul>

                <!-- IFEval interactive plots -->
                <div class="experiment-plots">
                    <div id="ifeval-preset-buttons" class="preset-buttons"></div>
                    <div id="ifeval-run-legend" class="run-legend"></div>

                    <h4>Rewards</h4>
                    <div class="plot-row plot-row-1x4">
                        <div class="plot-container" id="ifeval-plot-reward"></div>
                        <div class="plot-container" id="ifeval-plot-completion-len"></div>
                        <div class="plot-container" id="ifeval-plot-allcorr"></div>
                        <div class="plot-container" id="ifeval-plot-allwrong"></div>
                    </div>

                    <h4>Training Dynamics</h4>
                    <div class="plot-row plot-row-1x4">
                        <div class="plot-container" id="ifeval-plot-loss-grpo"></div>
                        <div class="plot-container" id="ifeval-plot-loss-sft"></div>
                        <div class="plot-container" id="ifeval-plot-kl"></div>
                        <div class="plot-container" id="ifeval-plot-entropy"></div>
                    </div>

                    <h4>Pass@k (step 200)</h4>
                    <div class="plot-row">
                        <div class="plot-container" id="ifeval-plot-passk-ifeval"></div>
                        <div class="plot-container" id="ifeval-plot-passk-ifbench_test"></div>
                    </div>

                    <div id="ifeval-preset-explanation" class="preset-explanation"></div>
                </div>

                <!-- IFEval comparison table (auto-generated) -->
                <div id="ifeval-comparison-table" class="results-table-container"></div>
            </section>
        </section>

        <section id="code">
            <h3>2.4 MBPP (Partial results)</h3>
            <p>
                We train on Mostly Basic Python Programs (<a href="https://huggingface.co/datasets/Muennighoff/mbpp">MBPP</a>) train split (n=374). This is a limited run using GRPO only. We set lr=1e-5, beta=0.001, group size G=8.
                </p>
                <p>Since the base model will continue writing new functions after it's output the (often correct) answer, we use a truncated verifier that clips model output at the second top-level <code>def</code>/<code>class</code> before running tests.
            </p>

            <div class="experiment-plots">
                <div id="code-preset-buttons" class="preset-buttons"></div>
                <div id="code-run-legend" class="run-legend"></div>
                <div id="code-preset-explanation" class="preset-explanation"></div>

                <h4>Rewards</h4>
                <div class="plot-row plot-row-1x4">
                    <div class="plot-container" id="code-plot-reward"></div>
                    <div class="plot-container" id="code-plot-completion-len"></div>
                    <div class="plot-container" id="code-plot-allcorr"></div>
                    <div class="plot-container" id="code-plot-allwrong"></div>
                </div>

                <h4>Training Dynamics</h4>
                <div class="plot-row plot-row-1x4">
                    <div class="plot-container" id="code-plot-loss-grpo"></div>
                    <div class="plot-container" id="code-plot-kl"></div>
                    <div class="plot-container" id="code-plot-entropy"></div>
                </div>

            </div>

            <!-- Code comparison table (auto-generated) -->
            <div id="code-comparison-table" class="results-table-container"></div>
        </section>

        <section id="going-further">
            <h3>2.5 Going further</h3>

            <h4>Distillation experiments (GKD / OPSD)</h4>
                <p>
                    It would be interesting to see how the methods here compare to "dense" on-policy distillation methods like generalized knowledge distillation, or <a href="https://arxiv.org/pdf/2306.13649">GKD</a> (in particular in conjunction with RL methods; see Section 3.2 in that paper). </p>
                    <p>In the past couple of years, several papers have used a form of self-distillation where the self-teacher is the same model with additional privileged information (<a href="https://arxiv.org/pdf/2601.20802">Reinforcement Learning via Self-Distillation</a>, <a href="https://arxiv.org/pdf/2601.18734">On-policy self-distillation</a>). </p>
                    <p>We ran preliminary tests using these methods on GSM8k and IFEval without success. Further investigation likely requires more hyperparameter tuning or a more capable initial model. We should also note that for this sort of exercise, the benefits of the efficient generation machinery in this package are less clear. On the one hand, we can generate completions and compute logprobs for the trainer and (self-)teacher using vLLM. On the other hand, when computing the trainer forward_backward we might need to pass logprobs not only for the selected tokens as we do here, but for the entire vocab, or at least some truncated version of it, and passing payload to the Ray actor via RPC can be slow.
                </p>
        </section>        

        <section id="appendix" class="appendix">
            <h2>Appendix</h2>
        
            <section id="notes">
                <h3>Notes</h3>
                <ol>
                    <li>KL divergence: we use the Schulman "k3" estimator: given $r = \log(\pi_{\text{ref}} / \pi_\theta)$, the quantity $\exp(r) - r - 1$ is an unbiased estimator of $\text{KL}(\pi_\theta \| \pi_{\text{ref}})$, the KL (policy w.r.t. reference). This penalizes the policy for placing probability mass where the reference does not, which is the standard choice in RLHF/GRPO &mdash; it discourages the policy from straying too far from the reference in a mode-seeking direction.</li>
                    <li>Precision: we use float16 (with dynamic loss scaling) for training rather than bfloat16. On A100 GPUs, we observed that bfloat16 training ran approximately 2x slower for the 1.7B model due to reduced throughput in the GRPO loss computation. One GRPO run (lr=1e-5) was initially launched with bfloat16 and had to be restarted with float16 after observing the slowdown. We did not observe numerical instability from float16 at this model scale.</li>
                </ol>
            </section>
        </section>

        
        <section id="staleness-appendix">
                <h3>Staleness conditions</h3>
                <details class="staleness-details">
                    <summary>Staleness diagrams</summary>
                    <div class="staleness-diagrams-combined">
                        <div class="diagrams-stack">
                        <svg viewBox="0 0 380 330" xmlns="http://www.w3.org/2000/svg">
                            <!-- Background -->
                            <rect width="380" height="330" fill="#fafafa"/>

                            <!-- Shared row labels (left side) -->
                            <text x="8" y="52" font-family="Roboto, sans-serif" font-size="9" fill="#666">Trainer</text>
                            <text x="8" y="72" font-family="Roboto, sans-serif" font-size="9" fill="#666">Rollout</text>
                            <text x="8" y="92" font-family="Roboto, sans-serif" font-size="9" fill="#666">Accepts</text>

                            <!-- ===== DIAGRAM 1: Strict on-policy ===== -->
                            <g transform="translate(0, 10)">
                                <!-- Grid lines -->
                                <g stroke="#e0e0e0" stroke-width="0.5">
                                    <line x1="55" y1="30" x2="55" y2="95"/>
                                    <line x1="165" y1="30" x2="165" y2="95"/>
                                    <line x1="275" y1="30" x2="275" y2="95"/>
                                </g>

                                <!-- Trainer row: v0, v1 -->
                                <rect x="55" y="40" width="110" height="16" rx="2" fill="#a371f7"/>
                                <text x="110" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="165" y="40" width="110" height="16" rx="2" fill="#8b5cf6"/>
                                <text x="220" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>

                                <!-- Rollout row -->
                                <rect x="55" y="58" width="110" height="16" rx="2" fill="#3fb950"/>
                                <text x="110" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="165" y="58" width="110" height="16" rx="2" fill="#22c55e"/>
                                <text x="220" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>

                                <!-- Accepts row -->
                                <rect x="55" y="76" width="110" height="16" rx="2" fill="#58a6ff"/>
                                <text x="110" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="165" y="76" width="110" height="16" rx="2" fill="#3b82f6"/>
                                <text x="220" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>

                                <!-- Sync annotation -->
                                <line x1="165" y1="35" x2="165" y2="95" stroke="#22c55e" stroke-width="1.5"/>
                                <text x="165" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#22c55e" text-anchor="middle">sync+optim</text>
                            </g>

                            <!-- ===== DIAGRAM 2: With staleness=1 ===== -->
                            <g transform="translate(0, 115)">
                                <!-- Grid lines -->
                                <g stroke="#e0e0e0" stroke-width="0.5">
                                    <line x1="55" y1="30" x2="55" y2="95"/>
                                    <line x1="110" y1="30" x2="110" y2="95"/>
                                    <line x1="165" y1="30" x2="165" y2="95"/>
                                    <line x1="220" y1="30" x2="220" y2="95"/>
                                    <line x1="275" y1="30" x2="275" y2="95"/>
                                </g>

                                <!-- Trainer row: v0, v1, v2, v3 -->
                                <rect x="55" y="40" width="55" height="16" rx="2" fill="#a371f7"/>
                                <text x="82" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="110" y="40" width="55" height="16" rx="2" fill="#8b5cf6"/>
                                <text x="137" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>
                                <rect x="165" y="40" width="55" height="16" rx="2" fill="#7c3aed"/>
                                <text x="192" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v2</text>
                                <rect x="220" y="40" width="55" height="16" rx="2" fill="#6d28d9"/>
                                <text x="247" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v3</text>

                                <!-- Rollout row: v0, v2 -->
                                <rect x="55" y="58" width="110" height="16" rx="2" fill="#3fb950"/>
                                <text x="110" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="165" y="58" width="110" height="16" rx="2" fill="#22c55e"/>
                                <text x="220" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v2</text>

                                <!-- Accepts row -->
                                <rect x="55" y="76" width="55" height="16" rx="2" fill="#58a6ff"/>
                                <text x="82" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="110" y="76" width="55" height="16" rx="2" fill="#3b82f6"/>
                                <text x="137" y="87" font-family="Roboto, sans-serif" font-size="7" fill="white" text-anchor="middle" font-weight="500">v0,v1</text>
                                <rect x="165" y="76" width="55" height="16" rx="2" fill="#2563eb"/>
                                <text x="192" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v2</text>
                                <rect x="220" y="76" width="55" height="16" rx="2" fill="#1d4ed8"/>
                                <text x="247" y="87" font-family="Roboto, sans-serif" font-size="7" fill="white" text-anchor="middle" font-weight="500">v2,v3</text>

                                <!-- Annotations -->
                                <line x1="110" y1="35" x2="110" y2="95" stroke="#a371f7" stroke-width="1.5"/>
                                <text x="110" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#a371f7" text-anchor="middle">optim</text>
                                <line x1="165" y1="35" x2="165" y2="95" stroke="#22c55e" stroke-width="1.5"/>
                                <text x="165" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#22c55e" text-anchor="middle">sync+optim</text>
                                <line x1="220" y1="35" x2="220" y2="95" stroke="#a371f7" stroke-width="1.5"/>
                                <text x="220" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#a371f7" text-anchor="middle">optim</text>
                                <line x1="275" y1="35" x2="275" y2="95" stroke="#22c55e" stroke-width="1.5"/>
                                <text x="275" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#22c55e" text-anchor="middle">sync+optim</text>
                            </g>

                            <!-- ===== DIAGRAM 3: Starvation ===== -->
                            <g transform="translate(0, 220)">
                                <!-- Grid lines -->
                                <g stroke="#e0e0e0" stroke-width="0.5">
                                    <line x1="55" y1="30" x2="55" y2="95"/>
                                    <line x1="110" y1="30" x2="110" y2="95"/>
                                    <line x1="165" y1="30" x2="165" y2="95"/>
                                    <line x1="220" y1="30" x2="220" y2="95"/>
                                    <line x1="275" y1="30" x2="275" y2="95"/>
                                </g>

                                <!-- Trainer row: v0, v1 then nothing -->
                                <rect x="55" y="40" width="55" height="16" rx="2" fill="#a371f7"/>
                                <text x="82" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="110" y="40" width="55" height="16" rx="2" fill="#8b5cf6"/>
                                <text x="137" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>

                                <!-- Rollout row: v0 then nothing -->
                                <rect x="55" y="58" width="110" height="16" rx="2" fill="#3fb950"/>
                                <text x="110" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>

                                <!-- Accepts row: v0, then DEADLOCK -->
                                <rect x="55" y="76" width="55" height="16" rx="2" fill="#58a6ff"/>
                                <text x="82" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="110" y="76" width="55" height="16" rx="2" fill="#f85149" fill-opacity="0.2" stroke="#f85149" stroke-width="1.5" stroke-dasharray="3,2"/>
                                <text x="137" y="87" font-family="Roboto, sans-serif" font-size="7" fill="#f85149" text-anchor="middle" font-weight="500">DEAD</text>

                                <!-- Annotation -->
                                <line x1="110" y1="35" x2="110" y2="95" stroke="#f85149" stroke-width="1.5"/>
                                <text x="110" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#f85149" text-anchor="middle">optim&rarr;deadlock</text>

                                <!-- Time axis (only on bottom diagram) -->
                                <line x1="40" y1="100" x2="285" y2="100" stroke="#999" stroke-width="0.5"/>
                                <text x="55" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">0</text>
                                <text x="110" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">8</text>
                                <text x="165" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">16</text>
                                <text x="220" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">24</text>
                                <text x="275" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">32</text>
                            </g>
                        </svg>
                    </div>
                    <div class="diagrams-captions">
                        <div class="diagram-caption-item">
                            <strong>Strict on-policy</strong><br>
                            <code>staleness=0, optim=16, sync=16</code><br>
                            All advance together. No starvation, but no parallelism.
                        </div>
                        <div class="diagram-caption-item">
                            <strong>With staleness</strong><br>
                            <code>staleness=1, optim=8, sync=16</code><br>
                            Trainer advances every 8. Rollout syncs to v2 at t=16. No v1 data, but no starvation.
                        </div>
                        <div class="diagram-caption-item">
                            <strong>Starvation</strong><br>
                            <code>staleness=0, optim=8, sync=16</code><br>
                            At t=8, trainer needs v1 but rollout is at v0 &rarr; deadlock.
                        </div>
                    </div>
                    </div>
                </details>
            </section>



    </article>

    <script src="viewer.js"></script>
    <script src="gsm8k_plots.js?v=1"></script>
    <script src="ifeval_plots.js?v=1"></script>
    <script src="code_plots.js?v=1"></script>
    <script>hljs.highlightAll();</script>
    <script>
    // Figure stepper widgets
    document.querySelectorAll('.figure-stepper').forEach(stepper => {
        const imgs = stepper.querySelectorAll('.stepper-img');
        const prev = stepper.querySelector('.stepper-prev');
        const next = stepper.querySelector('.stepper-next');
        const counter = stepper.querySelector('.stepper-counter');
        let idx = 0;
        const n = imgs.length;

        function update() {
            imgs.forEach((img, i) => img.classList.toggle('active', i === idx));
            counter.textContent = (idx + 1) + ' / ' + n;
            prev.disabled = idx === 0;
            next.disabled = idx === n - 1;
        }

        prev.addEventListener('click', () => { if (idx > 0) { idx--; update(); } });
        next.addEventListener('click', () => { if (idx < n - 1) { idx++; update(); } });

        // Keyboard: left/right arrows when stepper is in viewport
        stepper.setAttribute('tabindex', '0');
        stepper.addEventListener('keydown', e => {
            if (e.key === 'ArrowLeft') { prev.click(); e.preventDefault(); }
            if (e.key === 'ArrowRight') { next.click(); e.preventDefault(); }
        });

        update();
    });
    </script>
</body>
</html>
