<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RLVR</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;0,600;1,400&family=Source+Code+Pro:wght@400;500&display=swap" rel="stylesheet">
    <!-- D3.js for plots -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
</head>
<body>
    <article>
        <header>
            <h1>RLVR Experiments</h1>
        </header>

        <p>
            Github repo: <a href="https://github.com/halflearned/rlvr-experiments">halflearned/rlvr-experiments</a>
        </p>

        <section class="intro">
            <p>The post has two parts. Part 1 gives an overview of the package and typical usage. Part 2 presents a few experiments we ran with it.</p>
        </section>

        <section id="system-architecture">
            <h3>System overview</h3>

            <p>The main type of problem this package focuses on is Monte Carlo-based RLVR algorithm such as Group Relative Policy Optimzation (GRPO) and its variants. For this sort of problem, the canonical setup is a system that runs in a <b>producer-consumer pipeline</b>:
            </p>
            <p>The "producer" is a loop that samples prompts, generates completions using a copy of the model, verifies these completions, and puts all results in a buffer. </p>
                <p>The "consumer" pops from the buffer, waits until there are enough samples, batches samples together and trains its own model copy. </p>
                <p>Once the model weights are updated, trainer's new weights may be synced to the producer's weights to keep generations on policy</p>   
            <p>This sort of design, decoupling generation from training, has been used in <a href=https://arxiv.org/pdf/2501.12599>Kimi 1.5</a> and taken up by modern RLVR packages like <a href="https://pytorch.org/blog/introducing-torchforge/">torchforge</a>. 

            <p>Let's work through a typical flow.</p>

            </p>
            <h3>Components</h3>

            <p>A typical GRPO-like flow will have the following components</p>

            <ul>
                <li><strong>Trainer</strong> (TorchTitan): holds the policy model, runs forward/backward, steps the optimizer</li>
                <li><strong>Rollout engine</strong> (vLLM): generates completions from prompts</li>
                <li><strong>Reference model</strong> (vLLM): computes log-probabilities for the KL penalty</li>
                <li><strong>Verifier pool</strong>: checks completions against ground truth, returns binary rewards</li>
                <li><strong>Data iterator</strong>: serves prompts, tracks which are pending/in-flight/done</li>
            </ul>

            <p>The trainer's and vLLM engines's weights can be sharded across GPUs for different parallelism strategies. Weight sync happens periodically over NCCL, transferring updated policy weights from the trainer to the rollout and reference models.</p>
            <h3>Producer loop</h3>
            <p>
                The producer runs dozens of async workers concurrently, all competing for prompts from the data iterator. Each worker independently shepherds one prompt through the full pipeline: generation, verification, reference logprobs, then buffer.
            </p>
            <p>
                The producer has no visibility into the trainer's progress—it just keeps generating until the data is exhausted, pausing only during weight syncs. If a sample doesn't provide enough variation (all-wrong or all-correct) it is <em>filtered out</em> and not attempted again until the next epoch.
            </p>

            <figure>
                <img src="figures/producer.png" alt="Producer loop diagram">
            </figure>

            <details class="code-details">
                <summary>Producer code (<code>train_grpo_minimal.py</code>)</summary>
<pre><code class="language-python">async def produce_epoch():
    async def worker():
        while True:
            item = await data_iter.get_next_async()
            if item is None:
                return
            prompt_id = item["problem"]["prompt_id"]
            try:
                # Generate completions from current policy
                response = await rollout.generate_single(item["template"], **sampling_params)
                completions = [out.text for out in response.outputs]
                rollout_sample = RolloutSample.from_vllm(response, pad_token_id)

                # Score with verifier
                rewards = await verify(item["problem"], completions)

                # Skip if all rewards identical (no signal)
                if torch.tensor(rewards).std() &lt; 1e-6:
                    data_iter.mark_done(prompt_id)
                    return

                # Get reference model logprobs (for KL penalty)
                ref_logprobs = await compute_ref_logprobs(rollout_sample)

                # Push to buffer for consumer
                # Tag with trainer_version so consumer can check staleness later
                sample = TrainSample(rollout_sample, rewards, ref_logprobs,
                                     item_id=prompt_id, trainer_version=rollout.trainer_version)
                await buffer.put(sample, rollout.trainer_version, item_id=prompt_id)
            except Exception:
                data_iter.mark_failed(prompt_id)

    async with asyncio.TaskGroup() as tg:
        for _ in range(64):
            tg.create_task(worker())</code></pre>
            </details>

            <h3>Consumer loop</h3>
            <p>
                The consumer is a standard training loop: pull samples, batch them, forward/backward, optimizer step. Only two things are different. First, samples are checked for <em>staleness</em>: if the trainer has moved too far ahead of the weights used for generation a particular sample, that sample is (lazily) evicted and its prompt is re-queued for generation within this epoch. Second, after each optimizer step the trainer version increments, and consumed prompts are marked done so they won't be sampled again this epoch. Training usually happens in micro-batches that depend on the samples sequence length.
            </p>

            <figure>
                <img src="figures/consumer.png" alt="Consumer loop diagram">
            </figure>

            <p>A wrinkle: moving data both the forward and backward must happen within the same call.


            </p>

            <details class="code-details">
                <summary>Consumer code (<code>train_grpo_minimal.py</code>)</summary>
<pre><code class="language-python">for epoch in range(training.get("num_epochs", 1)):
    data_iter.new_epoch(seed=42 + epoch)
    producer = asyncio.create_task(produce_epoch())

    pending = []
    accum_count = 0

    while True:
        entry = await buffer.pop()
        if entry is None:
            break

        pending.append(entry.item)
        if len(pending) &lt; training["prompts_per_forward_backward"]:
            continue

        # ── Evict stale samples ────────────────────────────────────
        pending = evict_stale(pending)
        if len(pending) &lt; training["prompts_per_forward_backward"]:
            continue  # not enough left after eviction, keep collecting

        # ── Make batch ─────────────────────────────────────────────
        batch, stats = make_batch(pending, pad_token_id)
        group_sizes = [group_size] * len(pending)
        item_ids = [s.item_id for s in pending]
        pending = []

        # ── Compute advantages ────
        advantages = compute_grpo_advantages(batch.rewards, group_sizes=group_sizes)

        # ── Forward + backward (with micro-batching) ───────────────
        loss, _ = await trainer.forward_backward(
            loss_fn,
            batch.input_ids,
            loss_args=(batch.completion_ids, batch.ref_logprobs,
                       batch.logprobs, advantages),
            loss_kwargs={"padding_mask": batch.mask,
                         "prompt_lens": batch.prompt_lens,
                         "temperature": temperature},
            scale_loss=1.0 / accumulation_steps,
            micro_batch_size=get_micro_batch_size(stats.padded_seq_len),
        )

        # Mark items as consumed
        for item_id in item_ids:
            data_iter.mark_done(item_id)

        accum_count += 1
        if accum_count &lt; accumulation_steps:
            continue

        # ── Optimizer step ─────────────────────────────────────────
        grad_norm = await trainer.optim_step()
        accum_count = 0

        # ── Weight sync ──────────────────────────────────────────
        if trainer.version % sync_ref_every == 0:
            await sync_titan_to_vllm(trainer, reference,
                                     trainer_version=trainer.version)
        if trainer.version % sync_model_every == 0:
            await sync_titan_to_vllm(trainer, rollout,
                                     trainer_version=trainer.version)

        if training.get("max_steps") and trainer.version >= training["max_steps"]:
            break

    producer.cancel()
    await asyncio.gather(producer, return_exceptions=True)
    buffer.reset()</code></pre>
            </details>

            <h3>Weight sync</h3>
            <p>
                This is how the trainer's updated weights get to the rollout engine and reference model. The main challenge is copying efficiently when the trainer and rollout models may have weights sharded differently—the trainer might use FSDP-style sharding across 8 GPUs while the rollout uses tensor parallelism across 2. Different packages handle this differently. For example, <a href="#">torchforge</a> is developing a fast filesystem (<a href="https://github.com/meta-pytorch/torchstore">torchstore</a>) to save and load weights efficiently.
            </p>
            <p>
                Our strategy is to avoid disk entirely, relying on NCCL communication channels to broadcast weights directly between GPU memory. At initialization time, we create NCCL communicator groups (borrowing vLLM's StatelessProcessGroup class). This allows us to create overlapping "worlds" shared by each trainer-receiver pair. 
            </p>
            <ol>
                <li>Pause the rollout engine: block new requests behind an asyncio Event, and either wait for in-flight requests to drain (default) or abort them.</li>
                <li>Prepare the trainer for sync. Each trainer rank builds two things: a parameter map from HuggingFace parameter names to their local DTensor references, and a chunk plan—a list of (parameter name, offset, size) tuples dividing weights into ~100MB pieces. The chunk plan is sent to vLLM workers over Ray so both sides know how to pack and unpack.</li>
                <li>For each chunk: all trainer ranks participate in an all-gather to reconstruct full tensors from their shards (required because DTensor only exposes all-gather, not gather-to-one). Only rank 0 packs the result into a flat buffer and broadcasts over NCCL; vLLM workers receive into their own buffer and unpack into local model parameters, resharding as needed for their own parallelism layout.</li>
                <li>Update the rollout engine's trainer version so newly generated samples are tagged correctly.</li>
                <li>Resume generation: set the Event, blocked workers continue.</li>
            </ol>
            <p>
                With this strategy we complete the entire weight transfer for Qwen3-32B in 4.9s seconds or under (or a total of 7.5s if including waiting time for rollout engines to abort). For comparison, torchforge [reportedly](https://github.com/meta-pytorch/torchforge/issues/626) takes about 58 seconds with torchstore enabled.
            </p>

            <h3>Visualization</h3> We keep optional but extensive tracing capabilities that track the evolution of average rewards, generation speed, trainer effective MFU, and other quantities useful for debugging.


            <h3>Production-level run</h3> We tested a "production-level" run with Qwen3-32B (dense) using 2 nodes.... 

        </section>

        <section id="experiments">
            <h2>2. Basic experiment setup</h2>
            <p>
            All experiments use <a href="https://huggingface.co/Qwen/Qwen3-1.7B-Base">Qwen3-1.7B-Base</a> as the initial model. We'll train this model on simple math tasks (GSM8k), instruction-following (IFBench), and code (MBPP/APPS).
            </p>

            
            <p>For <a href="https://arxiv.org/abs/2503.20783">GRPO</a>, given a prompt $p$, we generate $G$ completions $o_{i}$ from the current model $\pi_{\theta}$, and compute:</p>

            $$
            J_{\text{GRPO}}(g)
            =
            \frac{1}{G} \sum_{i=1}^{G} \sum_{t=1}^{|o_i|}
            \left\{ \min \left[
                \frac{ \pi_{\theta}(o_t | q, o_{\lt t}) }{ \pi_{\theta_{\text{old}}}(o_t | q, o_{\lt t})  } \hat{A}_{i,t}
                ,
                \text{clip}\left(
                    \frac{ \pi_{\theta}(o_t | q, o_{\lt t}) }{ \pi_{\theta_{\text{old}}}(o_t | q, o_{\lt t})  },
                    1 - \varepsilon,
                    1 + \varepsilon
                 \right)
                \hat{A}_{i,t}
             \right] \right\}
            \tag{1}
            $$

            <p>
                where the advantage $\hat{A}_{i} = (r_i - \bar{r}) / \sigma_r$ is the z-scored reward for completion $i$ within its group, $\pi_{\theta_{\text{old}}}$ is the rollout policy used to generate the completions, and the probability ratio is clipped following <a href="https://arxiv.org/pdf/1707.06347">PPO-Clip</a>.
            </p>

            <p>
                Our second algorithm is a variant of <a href="https://arxiv.org/pdf/2503.14476">DAPO</a> (Decoupled Clip and Dynamic Sampling Policy Optimization), which uses the following objective,
            </p>

            $$
            J_{\text{DAPO}}(g)
            =
            \frac{1}{{\color{orange}\sum_{i} |o_i|}} \sum_{i=1}^{G} \sum_{t=1}^{|o_i|}
            \left\{ \min \left[
                \frac{ \pi_{\theta}(o_t | q, o_{\lt t}) }{ \pi_{\theta_{\text{old}}}(o_t | q, o_{\lt t})  } \hat{A}_{i,t}
                ,
                \text{clip}\left(
                    \frac{ \pi_{\theta}(o_t | q, o_{\lt t}) }{ \pi_{\theta_{\text{old}}}(o_t | q, o_{\lt t})  },
                    {\color{teal} 1 - \varepsilon_{\text{low}}},
                    {\color{teal} 1 + \varepsilon_{\text{high}}}
                 \right)
                \hat{A}_{i,t}
             \right] \right\}
            \tag{2}
            $$

            <p>
                the <span style="color: #e8a030;">normalization factor</span> divides by total tokens across the group rather than by $G$, removing a length bias that would otherwise discount longer completions. The <span style="color: teal;">clipping bounds</span> are asymmetric ($\varepsilon_{\text{low}} = 0.2$, $\varepsilon_{\text{high}} = 0.28$), allowing the policy more room to increase probability on rewarded completions than to decrease it on penalized ones.
            </p>

            <p>
                In either implementation, when rewards are constant, we discard the entire prompt. In addition, we ensure that every batch has the same number of prompts per step.
            </p>

            <p>
                We keep a small penalty for divergence with respect to a frozen reference policy. This is implemented using the "<a href="http://joschu.net/blog/kl-approx.html">Schulman</a>" approximation to the forward divergence $D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$.
            </p>

            <p>
To construct supervised fine-tuning targets, we apply rejection sampling fine-tuning (RFT; Yuan et al., 2023) from the base model itself. For each training problem, we sample 128 completions at temperature 1.0 and compute the per-problem pass rate. We retain only problems the model solves less than half the time and select one random correct completion per problem. Because the completions come from the base model rather than a stronger teacher, this is a form of self-distillation. When used as an auxiliary loss alongside GRPO or DAPO, the SFT term is weighted at 0.001.
            </p>

            <p>
                We have not experimented with adding an entropy bonus or other forms of reward shaping (e.g. length-based rewards).
            </p>

            <section id="gsm8k">
                <h3>2.2 GSM8K results</h3>
                <p>
                    We train on the GSM8K training set (n=7473).
                    All runs use Qwen3-1.7B-Base, group size $G=8$, and 200 steps. We ablate algorithm
                    (DAPO vs GRPO), auxiliary SFT loss, learning rate, and max staleness.
                </p>
                <p>

                </p>
                <p>
                    We have: (a) ablations under GSM8k -- GRPO vs DAPO, either vs SFT, staleness vs not.  Let's ignore staleness for now. The results there, as far as I can see, are: GRPO and DAPO when lr=1e-5 have the best results on GSM8k, however both see a decrease in MATH ability, and the decrease is much more pronounced for DAPO. However, DAPO+SFT maintains a high GSM8k score but also maintains MATH, the same is true for GRPO+SFT. For lre5-6, within the 200 step budget we still see a moderate increase in GSM8k ability, without a corresponding drop in MATH; there, the effect of an additional SFT boost is nothing. (b) We're missing a true "SFT only" run, the results presented there are not correct. It's also strange tha even when SFT is there, the SFT loss seems to be going *up*. (c) It's somewhat well documented that, for some models like Qwen2.5-xx-MATH, the pass-at-k does not increase with GRPO only for sufficiently large k. We see the same results here, and this is true regardless of SFT (d) For the ablation that ran so far, Staleness=0 vs Staleness > 0 seems to produce essentially the same results. Staleness=1 is particularly attractive because it means we can raise the MFU slightly for ~free.
                </p>
                <p>
                <ul>
                    <li>
                        GRPO vs DAPO: For a given set of hyperparameters, both perform very similarly. With the more aggressive learning rate lr=1e-5, rewards increase faster during training and achieve our best results on the GSM8k test set. However, MATH ability degrades, in particular for DAPO. Also, entropy decreases more rapidly with a higher learning rate, but generic performance as measured by MMLU remains essentially unchanged.</li>
                    <li>
                        Adding SFT: The effect of the auxiliary reward is somewhat ambiguous. For the more aggresive learning rate 1e-5, it mitigates some of the loss in MATH performance, in particular for DAPO. However for the slower learning rate 5e-6 it results in a further decrease in MATH performance. Perhaps it works as a regularizer for more aggressive learning rates, preventing it from overfitting on GSM8k. It does not prevent fast entropy decay in either case.
                    </li>
                    <li>
                        Pass@k: There have been several papers noting that performance in math capability, as measured by pass@k for some large k, doesn't improve with RVLR; see e.g., <a href="https://arxiv.org/pdf/2504.13837">Yue, Chen et al, 2015</a>. This would suggest that RLVR isn't well-suited for forcing a base model to learn new skills, only to reinforce those that it already has -- the "elicitation" theory. We also observe this here, as pass@128 seems to match the base model for both GSM8k and MATH for all model variations (see graph below). This may be a quirk of the Qwen family of models, as described <a href="https://www.interconnects.ai/p/reinforcement-learning-with-random">here</a>.
                    </li>
                    <li>
                        Staleness: In case generation is fast relative to training, our system allows for using older samples -- this can be done by simply not evicting samples older than a certain value of max_staleness. Since our weight sync mechanism is quite fast even for relatively sizable models.
                    </li>
                </ul>
                </p>

                <!-- GSM8K interactive plots -->
                <div class="experiment-plots">
                    <div id="gsm8k-preset-buttons" class="preset-buttons"></div>
                    <div id="gsm8k-run-legend" class="run-legend"></div>
                    <div id="gsm8k-preset-explanation" class="preset-explanation"></div>

                    <h4>Rewards</h4>
                    <div class="plot-row plot-row-2x2">
                        <div class="plot-container" id="gsm8k-plot-reward"></div>
                        <div class="plot-container" id="gsm8k-plot-completion-len"></div>
                        <div class="plot-container" id="gsm8k-plot-allcorr"></div>
                        <div class="plot-container" id="gsm8k-plot-allwrong"></div>
                    </div>

                    <h4>Training Dynamics</h4>
                    <div class="plot-row plot-row-2x2">
                        <div class="plot-container" id="gsm8k-plot-loss-grpo"></div>
                        <div class="plot-container" id="gsm8k-plot-loss-sft"></div>
                        <div class="plot-container" id="gsm8k-plot-kl"></div>
                        <div class="plot-container" id="gsm8k-plot-entropy"></div>
                    </div>
                </div>

                <!-- GSM8K comparison table (auto-generated) -->
                <div id="gsm8k-comparison-table" class="results-table-container"></div>

                <p> We computed pass@k curves show the probability that at least one of $k$ independent samples solves the problem, estimated from 128 completions per problem at temperature 1.0.
                </p>
                <p>
                    It's somewhat well documented that, for some models like Qwen2.5-xx-MATH, the pass-at-k does not increase with GRPO only for sufficiently large k. We see the same results here, and this is true regardless of SFT
                </p>

                <div class="experiment-plots">
                    <h4>Pass@k (step 200)</h4>
                    <div class="plot-row plot-row-2x2">
                        <div class="plot-container" id="gsm8k-plot-passk-gsm8k"></div>
                        <div class="plot-container" id="gsm8k-plot-passk-math"></div>
                        <div class="plot-container" id="gsm8k-plot-passk-aime"></div>
                        <div class="plot-container" id="gsm8k-plot-passk-beyondaime"></div>
                    </div>
                </div>


            <section id="staleness">
                <h3>Staleness</h3>
                <p>
                    If we allow completions generated from earlier model versions to be used, we can reduce...
                </p>
                <p>
                    After <span class="metric" data-run="dapo-lr5e6" data-key="total_steps"></span> steps, the strict on-policy run (<code>max_staleness=0</code>) achieves a final reward of <span class="metric" data-run="dapo-lr5e6" data-key="final_reward"></span>, while allowing one step of staleness (<code>max_staleness=1</code>) reaches <span class="metric" data-run="dapo-stale1" data-key="final_reward"></span> over <span class="metric" data-run="dapo-stale1" data-key="total_steps"></span> steps.
                </p>
            </section>

            </section>

            <section id="instruction-following">
                <h3>2.3 Instruction-following results</h3>
                <p>
                    We train on IFEval-multi-constraints (up to 5 constraints per prompt) and evaluate on
                    <a href="https://github.com/google-research/google-research/tree/master/instruction_following_eval">IFEval</a>
                    and <a href="https://huggingface.co/datasets/allenai/IFBench_test">IFBench_test</a>.
                    All runs use Qwen3-1.7B-Base, group size $G=16$, and 200 steps.
                </p>

                <!-- IFEval interactive plots -->
                <div class="experiment-plots">
                    <div id="ifeval-preset-buttons" class="preset-buttons"></div>
                    <div id="ifeval-run-legend" class="run-legend"></div>

                    <h4>Rewards</h4>
                    <div class="plot-row">
                        <div class="plot-container" id="ifeval-plot-reward"></div>
                        <div class="plot-container" id="ifeval-plot-allcorr"></div>
                        <div class="plot-container" id="ifeval-plot-allwrong"></div>
                        <div class="plot-container" id="ifeval-plot-completion-len"></div>
                    </div>

                    <h4>Training Dynamics</h4>
                    <div class="plot-row">
                        <div class="plot-container" id="ifeval-plot-loss-grpo"></div>
                        <div class="plot-container" id="ifeval-plot-kl"></div>
                        <div class="plot-container" id="ifeval-plot-entropy"></div>
                    </div>

                    <h4>Evaluation</h4>
                    <div class="plot-row">
                        <div class="plot-container" id="ifeval-plot-eval-ifeval"></div>
                        <div class="plot-container" id="ifeval-plot-eval-ifbench_test"></div>
                        <div class="plot-container" id="ifeval-plot-eval-gsm8k"></div>
                    </div>

                    <h4>Pass@k (step 200)</h4>
                    <div class="plot-row">
                        <div class="plot-container" id="ifeval-plot-passk-ifeval"></div>
                        <div class="plot-container" id="ifeval-plot-passk-ifbench"></div>
                    </div>

                    <div id="ifeval-preset-explanation" class="preset-explanation"></div>
                </div>

                <!-- IFEval comparison table (auto-generated) -->
                <div id="ifeval-comparison-table" class="results-table-container"></div>
            </section>
        </section>

        <section id="code">
            <h3>2.3 Code</h3>

        </section>

        <section id="going-further">
            <h3>Distillation experiments (GKD / OPSD)</h3>
                <p>
                    We also explored on-policy self-distillation (OPSD) and generalized knowledge distillation (GKD),
                    using either the model's own earlier checkpoint or a stronger Qwen3 instruct model as teacher.
                    Pure OPSD (distillation-only, no reward signal) consistently degraded GSM8k performance,
                    confirming that reward signal is essential. GRPO+GKD (combining reward with JSD distillation
                    in a single forward pass) maintained reward while adding the distillation term, and is a
                    promising direction for future work.
                </p>
        </section>        

        <section id="appendix" class="appendix">
            <h2>Appendix</h2>

            <section id="datasets">
                <h3>Dataset, verifiers, benchmarks.</h3>
                <p><strong>GSM8K:</strong> The MathVerifier achieves only 8-9% on base model completions compared to lm_eval's 66% strict-match accuracy. This discrepancy is intentional and correct. The base model often continues generating after providing the answer, producing additional unrelated Q&A pairs. MathVerifier extracts the <em>last</em> number in the response, which in these cases comes from the spurious continuation rather than the actual answer. lm_eval's strict-match, by contrast, extracts the <em>first</em> occurrence of "The answer is X." and ignores everything after. While lm_eval's approach is appropriate for evaluation (measuring whether the model <em>can</em> solve the problem), MathVerifier's stricter approach is the right training signal for RL: it rewards completions that both solve the problem correctly <em>and</em> terminate appropriately, rather than giving credit to responses that ramble indefinitely after stating the answer.</p>
                <p><strong>MATH:</strong></p>
            </section>
 
        
            <section id="notes">
                <h3>Notes</h3>
                <ol>
                    <li>Using a base model. We train from a base (pretrained-only) checkpoint to isolate the effect of our RL procedure from any prior alignment. Qwen3-1.7B-Base sits comfortably mid-range on all benchmarks of interest (55-60% GSM8k/MATH, 15-25% IFEval, 20-55% code), leaving headroom for improvement while remaining sensitive to degradation. In a production setting one would first distill chain-of-thought traces from a stronger model before applying RL (<a href="https://arxiv.org/pdf/2501.12948"></a>DeepSeek-R1</a> or <a href="https://arxiv.org/pdf/2203.14465">STaR</a>). </li>
                    <li>Formatting: </li>
                    <li>MFU</li>
                    <li>Kernels: LIGER kernel probably faster than TP=2</li>
                    <li>KL divergence: we use the Schulman "k3" estimator: given $r = \log(\pi_{\text{ref}} / \pi_\theta)$, the quantity $\exp(r) - r - 1$ is an unbiased estimator of $\text{KL}(\pi_\theta \| \pi_{\text{ref}})$, the "forward" KL (policy w.r.t. reference). This penalizes the policy for placing probability mass where the reference does not, which is the standard choice in RLHF/GRPO &mdash; it discourages the policy from straying too far from the reference in a mode-seeking direction.</li>
                    <li>Precision: we use float16 (with dynamic loss scaling) for training rather than bfloat16. On A100 GPUs, we observed that bfloat16 training ran approximately 2x slower for the 1.7B model due to reduced throughput in the GRPO loss computation. One GRPO run (lr=1e-5) was initially launched with bfloat16 and had to be restarted with float16 after observing the slowdown. We did not observe numerical instability from float16 at this model scale.</li>
                </ol>
            </section>
        </section>

        <section id="references">
            <h2>References</h2>
        </section>


                   <section id="staleness-appendix">
                <h3>Staleness conditions</h3>
                <!-- EMBED: staleness-diagrams -->
                <details class="staleness-details">
                    <summary>Staleness diagrams</summary>
                    <div class="staleness-diagrams-combined">
                        <div class="diagrams-stack">
                        <svg viewBox="0 0 380 330" xmlns="http://www.w3.org/2000/svg">
                            <!-- Background -->
                            <rect width="380" height="330" fill="#fafafa"/>

                            <!-- Shared row labels (left side) -->
                            <text x="8" y="52" font-family="Roboto, sans-serif" font-size="9" fill="#666">Trainer</text>
                            <text x="8" y="72" font-family="Roboto, sans-serif" font-size="9" fill="#666">Rollout</text>
                            <text x="8" y="92" font-family="Roboto, sans-serif" font-size="9" fill="#666">Accepts</text>

                            <!-- ===== DIAGRAM 1: Strict on-policy ===== -->
                            <g transform="translate(0, 10)">
                                <!-- Grid lines -->
                                <g stroke="#e0e0e0" stroke-width="0.5">
                                    <line x1="55" y1="30" x2="55" y2="95"/>
                                    <line x1="165" y1="30" x2="165" y2="95"/>
                                    <line x1="275" y1="30" x2="275" y2="95"/>
                                </g>

                                <!-- Trainer row: v0, v1 -->
                                <rect x="55" y="40" width="110" height="16" rx="2" fill="#a371f7"/>
                                <text x="110" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="165" y="40" width="110" height="16" rx="2" fill="#8b5cf6"/>
                                <text x="220" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>

                                <!-- Rollout row -->
                                <rect x="55" y="58" width="110" height="16" rx="2" fill="#3fb950"/>
                                <text x="110" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="165" y="58" width="110" height="16" rx="2" fill="#22c55e"/>
                                <text x="220" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>

                                <!-- Accepts row -->
                                <rect x="55" y="76" width="110" height="16" rx="2" fill="#58a6ff"/>
                                <text x="110" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="165" y="76" width="110" height="16" rx="2" fill="#3b82f6"/>
                                <text x="220" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>

                                <!-- Sync annotation -->
                                <line x1="165" y1="35" x2="165" y2="95" stroke="#22c55e" stroke-width="1.5"/>
                                <text x="165" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#22c55e" text-anchor="middle">sync+optim</text>
                            </g>

                            <!-- ===== DIAGRAM 2: With staleness=1 ===== -->
                            <g transform="translate(0, 115)">
                                <!-- Grid lines -->
                                <g stroke="#e0e0e0" stroke-width="0.5">
                                    <line x1="55" y1="30" x2="55" y2="95"/>
                                    <line x1="110" y1="30" x2="110" y2="95"/>
                                    <line x1="165" y1="30" x2="165" y2="95"/>
                                    <line x1="220" y1="30" x2="220" y2="95"/>
                                    <line x1="275" y1="30" x2="275" y2="95"/>
                                </g>

                                <!-- Trainer row: v0, v1, v2, v3 -->
                                <rect x="55" y="40" width="55" height="16" rx="2" fill="#a371f7"/>
                                <text x="82" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="110" y="40" width="55" height="16" rx="2" fill="#8b5cf6"/>
                                <text x="137" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>
                                <rect x="165" y="40" width="55" height="16" rx="2" fill="#7c3aed"/>
                                <text x="192" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v2</text>
                                <rect x="220" y="40" width="55" height="16" rx="2" fill="#6d28d9"/>
                                <text x="247" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v3</text>

                                <!-- Rollout row: v0, v2 -->
                                <rect x="55" y="58" width="110" height="16" rx="2" fill="#3fb950"/>
                                <text x="110" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="165" y="58" width="110" height="16" rx="2" fill="#22c55e"/>
                                <text x="220" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v2</text>

                                <!-- Accepts row -->
                                <rect x="55" y="76" width="55" height="16" rx="2" fill="#58a6ff"/>
                                <text x="82" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="110" y="76" width="55" height="16" rx="2" fill="#3b82f6"/>
                                <text x="137" y="87" font-family="Roboto, sans-serif" font-size="7" fill="white" text-anchor="middle" font-weight="500">v0,v1</text>
                                <rect x="165" y="76" width="55" height="16" rx="2" fill="#2563eb"/>
                                <text x="192" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v2</text>
                                <rect x="220" y="76" width="55" height="16" rx="2" fill="#1d4ed8"/>
                                <text x="247" y="87" font-family="Roboto, sans-serif" font-size="7" fill="white" text-anchor="middle" font-weight="500">v2,v3</text>

                                <!-- Annotations -->
                                <line x1="110" y1="35" x2="110" y2="95" stroke="#a371f7" stroke-width="1.5"/>
                                <text x="110" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#a371f7" text-anchor="middle">optim</text>
                                <line x1="165" y1="35" x2="165" y2="95" stroke="#22c55e" stroke-width="1.5"/>
                                <text x="165" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#22c55e" text-anchor="middle">sync+optim</text>
                                <line x1="220" y1="35" x2="220" y2="95" stroke="#a371f7" stroke-width="1.5"/>
                                <text x="220" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#a371f7" text-anchor="middle">optim</text>
                                <line x1="275" y1="35" x2="275" y2="95" stroke="#22c55e" stroke-width="1.5"/>
                                <text x="275" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#22c55e" text-anchor="middle">sync+optim</text>
                            </g>

                            <!-- ===== DIAGRAM 3: Starvation ===== -->
                            <g transform="translate(0, 220)">
                                <!-- Grid lines -->
                                <g stroke="#e0e0e0" stroke-width="0.5">
                                    <line x1="55" y1="30" x2="55" y2="95"/>
                                    <line x1="110" y1="30" x2="110" y2="95"/>
                                    <line x1="165" y1="30" x2="165" y2="95"/>
                                    <line x1="220" y1="30" x2="220" y2="95"/>
                                    <line x1="275" y1="30" x2="275" y2="95"/>
                                </g>

                                <!-- Trainer row: v0, v1 then nothing -->
                                <rect x="55" y="40" width="55" height="16" rx="2" fill="#a371f7"/>
                                <text x="82" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="110" y="40" width="55" height="16" rx="2" fill="#8b5cf6"/>
                                <text x="137" y="51" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v1</text>

                                <!-- Rollout row: v0 then nothing -->
                                <rect x="55" y="58" width="110" height="16" rx="2" fill="#3fb950"/>
                                <text x="110" y="69" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>

                                <!-- Accepts row: v0, then DEADLOCK -->
                                <rect x="55" y="76" width="55" height="16" rx="2" fill="#58a6ff"/>
                                <text x="82" y="87" font-family="Roboto, sans-serif" font-size="8" fill="white" text-anchor="middle" font-weight="500">v0</text>
                                <rect x="110" y="76" width="55" height="16" rx="2" fill="#f85149" fill-opacity="0.2" stroke="#f85149" stroke-width="1.5" stroke-dasharray="3,2"/>
                                <text x="137" y="87" font-family="Roboto, sans-serif" font-size="7" fill="#f85149" text-anchor="middle" font-weight="500">DEAD</text>

                                <!-- Annotation -->
                                <line x1="110" y1="35" x2="110" y2="95" stroke="#f85149" stroke-width="1.5"/>
                                <text x="110" y="30" font-family="Roboto, sans-serif" font-size="6" fill="#f85149" text-anchor="middle">optim&rarr;deadlock</text>

                                <!-- Time axis (only on bottom diagram) -->
                                <line x1="40" y1="100" x2="285" y2="100" stroke="#999" stroke-width="0.5"/>
                                <text x="55" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">0</text>
                                <text x="110" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">8</text>
                                <text x="165" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">16</text>
                                <text x="220" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">24</text>
                                <text x="275" y="108" font-family="Roboto, sans-serif" font-size="7" fill="#999" text-anchor="middle">32</text>
                            </g>
                        </svg>
                    </div>
                    <div class="diagrams-captions">
                        <div class="diagram-caption-item">
                            <strong>Strict on-policy</strong><br>
                            <code>staleness=0, optim=16, sync=16</code><br>
                            All advance together. No starvation, but no parallelism.
                        </div>
                        <div class="diagram-caption-item">
                            <strong>With staleness</strong><br>
                            <code>staleness=1, optim=8, sync=16</code><br>
                            Trainer advances every 8. Rollout syncs to v2 at t=16. No v1 data, but no starvation.
                        </div>
                        <div class="diagram-caption-item">
                            <strong>Starvation</strong><br>
                            <code>staleness=0, optim=8, sync=16</code><br>
                            At t=8, trainer needs v1 but rollout is at v0 &rarr; deadlock.
                        </div>
                    </div>
                    </div>
                </details>
            </section>



    </article>

    <script src="viewer.js"></script>
    <script src="gsm8k_plots.js?v=1"></script>
    <script src="ifeval_plots.js?v=1"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
