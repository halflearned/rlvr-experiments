run:
  name: qwen3_0_6b_grpo_dev
  description: trying to get grpo to work! :)


model:
  local_path: "./assets/hf/Qwen3-0.6B"  # currently not used

roles:
  - name: trainer
    kind: titan
    config:
      # my titan model takes this additional keyword.
      # if false then it doesn't create optimizers etc
      trainable: true

      profiling:
        enable_profiling: false
        save_traces_folder: "profile_trace"
        profile_freq: 100

      metrics:
        log_freq: 1
        enable_tensorboard: false
        save_tb_folder: "tb"

      # TODO: we should extract this from model local_path
      model:
        name: "qwen3"
        flavor: "0.6B"
        hf_assets_path: "./assets/hf/Qwen3-0.6B"

      optimizer:
        name: "AdamW"
        lr: 0.00001
        eps: 0.00000001

      lr_scheduler:
        warmup_steps: 2

      training:
        seq_len: 4096
        dtype: "bfloat16"
        mixed_precision_param: "bfloat16"
        mixed_precision_reduce: "float32"

      parallelism:  # we can extract this into a 'Parallelism' if useful, but don't need to if it isn't
        # note that we should use this to compute how many gpus we need
        data_parallel_replicate_degree: 1
        data_parallel_shard_degree: 1
        fsdp_reshard_after_forward: "default"
        tensor_parallel_degree: 2
        context_parallel_degree: 1
        # IMPORTANT: disable_loss_parallel must be true when computing custom losses
        # with TP>1, otherwise logits are sharded on vocab and cross_entropy is wrong
        disable_loss_parallel: true

      checkpoint:
        # I think we actually won't use this in the end, but alas.
        enable: true
        folder: "checkpoint"
        initial_load_in_hf: true
        interval: 500
        last_save_model_only: false
        export_dtype: "bfloat16"
        async_mode: "disabled"

      activation_checkpoint:
        mode: "none"
        selective_ac_option: "op"

      compile:
        # I also don't know what this does, need to figure out
        enable: false
        components:
          - "model"


  - name: reference
    kind: titan
    config:
      trainable: false

      model:
        name: "qwen3"
        flavor: "0.6B"
        hf_assets_path: "./assets/hf/Qwen3-0.6B"

      parallelism:  
        data_parallel_replicate_degree: 1
        data_parallel_shard_degree: 1
        fsdp_reshard_after_forward: "default"
        tensor_parallel_degree: 1 
        context_parallel_degree: 1

      training:
        seq_len: 4096
        dtype: "bfloat16"
        mixed_precision_param: "bfloat16"
        mixed_precision_reduce: "float32"

      checkpoint:
        # I think we actually won't use this in the end, but ok for now?
        enable: true
        folder: "checkpoint"
        initial_load_in_hf: true
        interval: 500
        last_save_model_only: false
        export_dtype: "bfloat16"
        async_mode: "disabled"

      activation_checkpoint:
        mode: selective
        selective_ac_option: op

      compile:
        enable: false
        components:
          - "model"

      
  - name: rollout
    kind: vllm

    config:
      model: "./assets/hf/Qwen3-0.6B"
      tensor_parallel_size: 1  # TP=1 is faster for small models - avoids NCCL overhead
      max_model_len: 1024  # Reduced from 4096 - smaller KV cache = faster
      gpu_memory_utilization: 0.90
      dtype: "bfloat16"
      logprobs_mode: "raw_logprobs"
      max_num_seqs: 100
      max_num_batched_tokens: 32768
      # Speed optimizations
      enable_prefix_caching: true  # Cache common prompt prefix across n=20 samples
      enable_chunked_prefill: true  # Better batching efficiency

      

sync:
  chunk_mb: 100

  wiring:
    - src: trainer
      dst: reference
    - src: trainer
      dst: rollout
    