run:
  name: qwen3_1_7b_math_vllm_ref

model:
  path: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"

tokenizer:
  pretrained_model_name_or_path: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"
  use_fast: false

training:
  num_epochs: 1
  iterations_per_epoch: null  # null = run full epoch until data exhausted
  max_staleness: 0  # Samples older than (current_version - max_staleness) are evicted and retried
  abort_in_flight: true  # If true, abort in-flight generations on sync (wastes GPU work). If false, wait for completion.
  checkpoint_interval: 50  # Save HF checkpoint every N steps

  # Fixed bucket sizes to avoid torch.compile recompilations
  # With max_tokens=1024 and prompts ~50-100 tokens, max seq_len ~1124
  seq_len_buckets: [512, 768, 1024, 1280]
  completion_len_buckets: [256, 512, 768, 1024]

  # Batching config (all counts are prompts, not completions)
  prompts_per_rollout_sync: 16    # sync vLLM weights after training on this many prompts
  prompts_per_reference_sync: 64  # sync reference model after training on this many prompts
  prompts_per_optim_step: 16      # effective batch size for optimizer (p prompts * n completions)
  prompts_per_forward_backward: 1  # how many prompts to wait until starting the forward_backward
  completions_per_micro_batch: 32  # micro-batch size for forward_backward
  #completions_per_micro_batch_reference: 32  # now unused since reference is vLLM

verifier:
  num_workers: 4
  timeout: 5.0  # seconds - first call needs ~1.5s for subprocess warmup, then <10ms

loss:
  beta: 0.05
  eps: 0.2

data:
  dataset: math
  level: [3, 4, 5]  # Levels 1-2 too easy, focus on harder problems

data_iter:
  system_prompt: "Solve the following math problem and provide the final answer inside \\boxed{}"
  assistant_prefix: "Let's think step by step."

sampling:
  temperature: 0.6
  top_p: 0.95
  top_k: 20
  max_tokens: 1024  # Reduced from 2048 - longer completions rarely correct anyway
  n: 32  # TODO: bump this up to 64! 32 is rookie numbers
  logprobs: 0


buffer:
  max_reads: 1
  maxsize: 0  # unbounded - backpressure via LoadAwareRouter instead

roles:
  - name: trainer
    kind: titan
    config:
      trainable: true

      profiling:
        enable_profiling: true
        save_traces_folder: "profile_trace"
        profile_freq: 100

      metrics:
        log_freq: 1
        enable_tensorboard: false
        save_tb_folder: "tb"

      model:
        name: "qwen3"
        flavor: "1.7B"
        hf_assets_path: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"

      optimizer:
        name: "AdamW"
        lr: 0.000005
        eps: 0.0001

      lr_scheduler:
        warmup_steps: 0
        decay_ratio: 0.0  # constant LR (no warmup, no decay)

      training:
        seq_len: 1280  # Must match vLLM max_model_len for rope_cache
        dtype: "bfloat16"
        mixed_precision_param: "bfloat16"
        mixed_precision_reduce: "float32"

      parallelism:
        data_parallel_replicate_degree: 1
        data_parallel_shard_degree: 1
        fsdp_reshard_after_forward: "default"
        tensor_parallel_degree: 2
        context_parallel_degree: 1
        disable_loss_parallel: false

      checkpoint:
        enable: true
        folder: "checkpoint"
        initial_load_in_hf: true
        interval: 500
        last_save_model_only: false
        export_dtype: "bfloat16"
        async_mode: "disabled"

      activation_checkpoint:
        mode: selective
        selective_ac_option: "op"

      compile:
        enable: true
        components:
          - "model"


  # Reference model as vLLM - much faster than Titan for logprobs-only workload
  # Uses prompt_logprobs feature with max_tokens=0
  # GPU allocation: trainer=2, reference=2, rollout=4 (total 8 GPUs)
  - name: reference
    kind: vllm
    config:
      model: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"
      max_concurrent_per_replica: 32   # Higher concurrency since logprobs are lighter than generation
      max_num_seqs: 256
      tensor_parallel_size: 1
      data_parallel_size: 3  # 2 replicas × 1 GPU = 2 GPUs
      max_model_len: 1281  # ~256 prompt + 1024 completion + 1 for max_tokens=1 in logprobs
      gpu_memory_utilization: 0.90
      dtype: "bfloat16"
      max_num_batched_tokens: 65536
      enable_prefix_caching: false  # Prefix caching may interfere with prompt_logprobs
      enable_chunked_prefill: true


  - name: rollout
    kind: vllm

    config:
      model: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"
      max_concurrent_per_replica: 8   # 8 prompts * 32 completions = 256 seqs per replica
      max_num_seqs: 256
      tensor_parallel_size: 1
      data_parallel_size: 3  # 4 replicas × 1 GPU = 4 GPUs
      max_model_len: 1280  # ~256 prompt + 1024 completion
      gpu_memory_utilization: 0.90
      dtype: "bfloat16"
      logprobs_mode: "raw_logprobs"
      max_num_batched_tokens: 65536
      enable_prefix_caching: true
      enable_chunked_prefill: true



sync:
  chunk_mb: 100

  wiring:
    - src: trainer
      dst: reference
    - src: trainer
      dst: rollout
