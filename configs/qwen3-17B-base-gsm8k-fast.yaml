run:
  name: qwen3_1_7b_grpo_fast

model:
  path: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"

tokenizer:
  pretrained_model_name_or_path: "Qwen/Qwen3-1.7B-Base"
  use_fast: false

training:
  num_epochs: 1  # Full epoch over GSM8K
  # Match working 0.6B/1.7B dummy config settings
  prompts_per_batch: 3    # 3 prompts * 16 completions = 48 sequences
  sync_reference_every: 5 # Use old naming like 0.6B dummy
  max_staleness: 1        # Match 0.6B dummy (was 4, caused slowdown)

verifier:
  num_workers: 4  # More workers for parallel verification
  # max_concurrent: 4
  # timeout: 5

loss:
  beta: 0.01
  eps: 0.2

data:
  dataset: gsm8k

data_iter:
  system_prompt: "Solve the following math problem and provide the final answer inside \\boxed{}"
  assistant_prefix: "Let's think step by step."

sampling:
  temperature: 0.6
  top_p: 0.95
  top_k: 20
  max_tokens: 512  # Match 0.6B dummy
  n: 16  # 16 completions per prompt (match 0.6B dummy)
  logprobs: 0

buffer:
  max_reads: 1  # Match 0.6B dummy

roles:
  # TRAINER: 1 GPU, no parallelism (baseline test)
  - name: trainer
    kind: titan
    config:
      trainable: true

      profiling:
        enable_profiling: false
        save_traces_folder: "profile_trace"
        profile_freq: 100

      metrics:
        log_freq: 1
        enable_tensorboard: false
        save_tb_folder: "tb"

      model:
        name: "qwen3"
        flavor: "1.7B"
        hf_assets_path: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"

      optimizer:
        name: "AdamW"
        lr: 0.00001
        eps: 0.0001

      lr_scheduler:
        warmup_steps: 50

      training:
        seq_len: 2048  # Match 0.6B dummy
        dtype: "bfloat16"
        mixed_precision_param: "bfloat16"
        mixed_precision_reduce: "float32"

      parallelism:
        data_parallel_replicate_degree: 1
        data_parallel_shard_degree: 1      # No FSDP
        fsdp_reshard_after_forward: "default"
        tensor_parallel_degree: 1          # No TP
        context_parallel_degree: 1
        disable_loss_parallel: true

      checkpoint:
        enable: false  # Disable for speed
        folder: "checkpoint"
        initial_load_in_hf: true
        interval: 500
        last_save_model_only: false
        export_dtype: "bfloat16"
        async_mode: "disabled"

      activation_checkpoint:
        mode: selective
        selective_ac_option: "op"

      compile:
        enable: true
        components:
          - "model"


  # REFERENCE: 1 GPU (no FSDP - avoids collective overhead for many small forward passes)
  # 1.7B fits easily on a single A100 80GB
  - name: reference
    kind: titan
    config:
      trainable: false

      model:
        name: "qwen3"
        flavor: "1.7B"
        hf_assets_path: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"

      parallelism:
        data_parallel_replicate_degree: 1
        data_parallel_shard_degree: 1   # Single GPU - no FSDP overhead!
        fsdp_reshard_after_forward: "default"
        tensor_parallel_degree: 1
        context_parallel_degree: 1

      training:
        seq_len: 2048  # Match 0.6B dummy
        dtype: "bfloat16"
        mixed_precision_param: "bfloat16"
        mixed_precision_reduce: "float32"

      checkpoint:
        enable: true  # Match 0.6B dummy
        folder: "checkpoint"
        initial_load_in_hf: true
        interval: 500
        last_save_model_only: false
        export_dtype: "bfloat16"
        async_mode: "disabled"

      activation_checkpoint:
        mode: none
        selective_ac_option: op

      compile:
        enable: false  # Disable compile for reference - variable seq lens cause recompiles
        components:
          - "model"


  # VLLM ROLLOUT: 1 GPU (match 0.6B dummy)
  - name: rollout
    kind: vllm

    config:
      model: "/efs/rlvr-experiments/assets/hf/Qwen3-1.7B-Base"
      tensor_parallel_size: 1
      data_parallel_size: 1
      max_model_len: 1024     # Match 0.6B dummy
      max_concurrent_per_replica: 8
      max_num_seqs: 100       # Match 0.6B dummy
      gpu_memory_utilization: 0.90
      dtype: "bfloat16"
      logprobs_mode: "raw_logprobs"
      max_num_batched_tokens: 32768
      enable_prefix_caching: true
      enable_chunked_prefill: true



sync:
  chunk_mb: 100

  wiring:
    - src: trainer
      dst: reference
    - src: trainer
      dst: rollout
