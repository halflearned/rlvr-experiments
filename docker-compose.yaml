# common configurations for both vllm and trainer services
x-common-config: &common-config
  ipc: host
  working_dir: /workspace
  volumes:
    - .:/workspace
  networks:
    - rlvr-net
  deploy:
    resources:
      reservations:
        devices:
          - capabilities: [gpu]

x-common-env: &common-env
  PYTHONPATH: /workspace/src

services:
  vllm:
    <<: *common-config
    image: rlvr-inference:latest
    container_name: inference-server
    entrypoint: []
    command: ["python3", "-u", "entrypoints/inference.py", "configs/inference.toml"]
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: 0,1 
      CUDA_VISIBLE_DEVICES: 0,1
      VLLM_WORKER_MULTIPROC: 1
      VLLM_LOGGING_LEVEL: DEBUG
    ports:
      - "8000:8000"

  trainer:
    <<: *common-config 
    image: rlvr-experiments:latest
    container_name: titan-trainer
    command: torchrun --standalone --nnodes=1 --nproc_per_node=2 entrypoints/training.py configs/training.toml
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: 2,3
      CUDA_VISIBLE_DEVICES: 2,3
      VLLM_BASE_URL: http://vllm:8000/v1

networks:
  rlvr-net:
    driver: bridge