--- a/lm_eval/models/vllm_causallms.py
+++ b/lm_eval/models/vllm_causallms.py
@@ -263,17 +263,19 @@ class VLLM(TemplateLM):
             # flatten results
             return undistribute(results)

+        # Convert token IDs to TokensPrompt format for vLLM 0.11.0 compatibility
+        prompts = [{"prompt_token_ids": r} for r in requests]
         if self.lora_request is not None:
             outputs = self.model.generate(
-                prompt_token_ids=requests,
+                prompts=prompts,
                 sampling_params=sampling_params,
                 use_tqdm=True if self.batch_size == "auto" else False,
                 lora_request=self.lora_request,
             )
         else:
             outputs = self.model.generate(
-                prompt_token_ids=requests,
+                prompts=prompts,
                 sampling_params=sampling_params,
                 use_tqdm=True if self.batch_size == "auto" else False,
             )
